<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Local LLM Setup Guide</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css">
    <style>
        body {
            font-family: system-ui, -apple-system, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
        }
        h1, h2, h3 {
            color: #2c5282;
        }
        .setup-container {
            background: #f7fafc;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }
        .step {
            background: white;
            border-left: 4px solid #4299e1;
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 4px 4px 0;
        }
        code {
            background: #edf2f7;
            padding: 2px 4px;
            border-radius: 4px;
            font-family: monospace;
        }
        .warning {
            background: #fff5f5;
            border-left: 4px solid #f56565;
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 4px 4px 0;
        }
        .code-block {
            background: #2d3748;
            color: #e2e8f0;
            padding: 15px;
            border-radius: 4px;
            margin: 10px 0;
            overflow-x: auto;
        }
        .toggle-section {
            cursor: pointer;
            padding: 10px;
            background: #e6f6ff;
            border-radius: 4px;
            margin: 10px 0;
        }
        .hidden {
            display: none;
        }
    </style>
</head>
<body>
    <h1>Local LLM Environment Setup Guide</h1>

    <div class="setup-container">
        <h2>Introduction</h2>
        <p>In line with experimenting with LLMs locally, this guide details setting up a local lab environment. Using a new M4 Mini with 64 GB of unified memory enables running substantial models on the local network. To address remote access needs, the setup incorporates Tailscale for private networking via Wireguard, creating a private mesh that allows secure LLM access from anywhere without public internet exposure.</p>
    </div>
    
    <div class="setup-container">
        <h2>System Overview</h2>
        <p>This guide describes how to set up a local LLM environment using a Mac Mini as the server, 
           with both local and remote access capabilities.</p>
        
        <div class="diagram-container">
            <h3>Physical Architecture</h3>
            <img src="llm-setup.png" alt="Physical Architecture Diagram showing Mac Mini M4, router, and devices">
            <p class="diagram-caption">Physical setup showing Mac Mini M4 (64GB) connected to local network and devices</p>
        </div>


        <h3>Key Components:</h3>
        <ul>
            <li><strong>Ollama:</strong> Running on Mac Mini to serve LLM models</li>
            <li><strong>Tailscale:</strong> For secure network connectivity</li>
            <li><strong>LLM CLI:</strong> Command-line interface for model interaction</li>
            <li><strong>Enchanted:</strong> iOS client for mobile access</li>
            <li><strong>UV:</strong>Python package and project manager, written in Rust</li>
        </ul>
    </div>

    <style>
        .diagram-container {
            background: white;
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }
        .diagram-caption {
            text-align: center;
            color: #4a5568;
            font-style: italic;
            margin-top: 10px;
        }
    </style>

    <div class="setup-container">
        <h2>Prerequisites</h2>
        <div class="requirements-grid">
            <div class="requirement-card">
                <h3>Hardware</h3>
                <ul>
                    <li>Mac Mini M4 with 64GB unified memory</li>
                    <li>iOS device for mobile access</li>
                    <li>Laptop</li>
                    <li>Local router with Ethernet/WiFi</li>
                </ul>
            </div>
            <div class="requirement-card">
                <h3>Network</h3>
                <ul>
                    <li>Stable internet connection</li>
                    <li>Administrator access on all devices</li>
                </ul>
            </div>
        </div>
    </div>

    <style>
        .requirements-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }
        .requirement-card {
            background: white;
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            padding: 20px;
        }
        .requirement-card h3 {
            margin-top: 0;
            color: #2d3748;
            border-bottom: 2px solid #e2e8f0;
            padding-bottom: 10px;
        }
    </style>

    <div class="setup-container">
        <h2>Step-by-Step Setup Instructions</h2>
        
        <div class="step">
            <h3>1. Install Ollama on Mac Mini</h3>
            <p>Download and install Ollama from the official website:</p>
            <pre><code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh</code></pre>
            <p>Verify installation:</p>
            <pre><code class="language-bash">ollama --version</code></pre>
        </div>

        <div class="step">
            <h3>2. Pull LLM Models</h3>
            <p>Download your preferred models. For example:</p>
            <pre><code class="language-bash"># Pull recommended models
ollama pull llama3.2-vision:latest
ollama pull smollm2:latest
ollama pull Qwen2.5-Coder:latest
ollama pull llama3.3:latest</code></pre>
        </div>

        <div class="step">
            <h3>3. Setup Tailscale</h3>
            <ol>
                <li>Download Tailscale from <a href="https://tailscale.com/download" target="_blank">tailscale.com</a></li>
                <li>Install on both Mac Mini and iOS device</li>
                <li>Sign in with your Tailscale account on both devices</li>
                <li>Note the Tailscale IP of your Mac Mini</li>
            </ol>
        </div>
        <div class="step">
            <h3>4. Install <a href="https://astral.sh/blog/uv">UV Package Manager</a></h3>
            <p>Install UV using the official installation script:</p>
            <pre><code class="language-bash">curl -LsSf https://astral.sh/uv/install.sh | sh</code></pre>
            <p>Verify installation:</p>
            <pre><code class="language-bash">uv --version</code></pre>
        </div>

        <div class="step">
            <h3>5. Install LLM CLI Tool with UV</h3>
            <p>Install Simon Willison's LLM tool on my laptop using UV and use the <a href="https://github.com/taketwo/llm-ollama">llm-ollama</a> to connect to ollama:</p>
            <pre><code class="language-bash"># Create and activate new virtual environment
uv venv .venv
source .venv/bin/activate

# Install LLM and Ollama plugin
uv pip install llm llm-ollama</code></pre>
            
            <p>Configure LLM with UV for Ollama access:</p>
            <pre><code class="language-bash"># Set up alias for easy access
alias ll33='OLLAMA_HOST=http://[tailscale-ip]:11434 uvx --with llm --with llm-ollama llm -m llama3.3'

# Test the connection
ll33 "Hello, world!"</code></pre>
        </div>


        <div class="step">
            <h3>6. Setup Enchanted iOS App</h3>
            <ol>
                <li>Download Enchanted from the App Store</li>
                <li>Open the app and go to Settings</li>
                <li>Add new server with your Tailscale IP</li>
                <li>Configure the endpoint: http://[tailscale-ip]:11434/api</li>
            </ol>
        </div>
    </div>

    <div class="warning">
        <h3>‚ö†Ô∏è Important Security Notes</h3>
        <ul>
            <li>Ensure your Tailscale network is properly configured for security</li>
            <li>Keep your Ollama installation updated</li>
            <li>Regularly update your LLM models</li>
            <li>Monitor system resources on your Mac Mini</li>
        </ul>
    </div>

    <div class="setup-container">
        <h2>Summary & Lessons Learned.</h2>
        <p>With this setup I can reach my private llm setup from anywhere on any device I choose to secure.  Tailscale makes it easy to connect peer-to-peer wireguard connections for private networking allowing me to access self-hosted models without making them publically available or opening up my network attack surface.  I can now grab and test new models as quickly as I can find them!  The Mac Mini is tiny, quiet and sips power, at some point I may want to upgrade to a serious GPU system but for now it meets my needs very well. I'm starting to get the hang of uv and using it to quickly test new packages for python has also been fun.</p>
    </div>

 <div class="setup-container">
        <h2>Troubleshooting</h2>
        <div class="toggle-section" onclick="toggleSection('network-issues')">
            üîç Network Connectivity Issues
        </div>
        <div id="network-issues" class="hidden">
            <ul>
                <li>Verify Tailscale is running on both devices</li>
                <li>Check Ollama service is running: <code>sudo lsof -i :11434</code></li>
                <li>Test connectivity: <code>ping [tailscale-ip]</code></li>
                <li>Verify no firewall blocking: <code>sudo lsof -i :11434</code></li>
                <li>By default MacOS will put the Mac Mini to sleep when not in use, make sure to change your energy settings to make sure the system is always available.</li>
            </ul>
        </div>

        <div class="toggle-section" onclick="toggleSection('model-issues')">
            üîç Model Loading Issues
        </div>
        <div id="model-issues" class="hidden">
            <ul>
                <li>Check model status: <code>ollama list</code></li>
                <li>Verify disk space: <code>df -h</code></li>
                <li>Check model pulling: <code>ollama pull [model] --verbose</code></li>
            </ul>
        </div>
    </div>
    <script>
        function toggleSection(id) {
            const section = document.getElementById(id);
            section.classList.toggle('hidden');
        }

        // Initialize Prism.js for syntax highlighting
        Prism.highlightAll();
    </script>
</body>
</html>